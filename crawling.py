import asyncio
import json
import time
import glob
import os
from playwright.async_api import async_playwright

MAX_COUNT = 1000

CATEGORY_LIST = [
    {"id": "18254183", "name": "ì—¬ì„±_ë ˆì¸ë¶€ì¸ "},
    {"id": "18255502", "name": "ì—¬ì„±_ë©”ë¦¬ì œì¸ìŠˆì¦ˆ"}
]

async def scrape_category(category_id, category_name):
    results = []
    seen_links = set()
    base_filename = f"danawa_{category_name}_results"

    for file in sorted(glob.glob(f"{base_filename}_*.json")):
        with open(file, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                for item in data:
                    seen_links.add(item["ë§í¬"])
            except Exception as e:
                print(f"â— íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {file} â†’ {e}")

    print(f"ğŸ” {category_name} ê¸°ì¡´ ìˆ˜ì§‘ëœ ìƒí’ˆ ìˆ˜: {len(seen_links)}ê°œ")
    file_index = len(seen_links) // 100

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        detail_page = await browser.new_page()

        category_url = f"https://prod.danawa.com/list/?cate={category_id}"
        await page.goto(category_url)
        await page.wait_for_timeout(3000)

        try:
            await page.click('a.tab_link.tab_compare')
            await page.wait_for_timeout(2000)
            print("âœ… ê°€ê²©ë¹„êµ íƒ­ í´ë¦­ ì„±ê³µ")
        except Exception as e:
            print(f"â— ê°€ê²©ë¹„êµ íƒ­ í´ë¦­ ì‹¤íŒ¨: {e}")

        try:
            await page.click('a.type_list[title*="ë¦¬ìŠ¤íŠ¸í˜•"]')
            await page.wait_for_timeout(1500)
            print("âœ… ë¦¬ìŠ¤íŠ¸í˜• íƒ­ í´ë¦­ ì„±ê³µ")
        except Exception as e:
            print(f"â— ë¦¬ìŠ¤íŠ¸í˜• íƒ­ í´ë¦­ ì‹¤íŒ¨: {e}")

        page_set = 0
        while True:
            page_set += 1
            print(f"\nğŸ“¦ [{category_name}] {page_set}ë²ˆì§¸ í˜ì´ì§€ ì„¸íŠ¸ íƒìƒ‰ ì¤‘...")

            for i in range(1, 11):
                try:
                    page_selector = f'div.number_wrap a.num:has-text("{i}")'
                    if await page.query_selector(page_selector):
                        await page.click(page_selector)
                        await page.wait_for_timeout(2500)

                        items = await page.query_selector_all("div.main_prodlist > ul > li.prod_item")
                        product_list = []

                        for item in items:
                            title_el = await item.query_selector("p.prod_name a")
                            price_el = await item.query_selector("p.price_sect strong")
                            if title_el and price_el:
                                title = await title_el.inner_text()
                                link = await title_el.get_attribute("href")
                                price = await price_el.inner_text()

                                if link and link not in seen_links:
                                    seen_links.add(link)
                                    product_list.append({
                                        "title": title.strip(),
                                        "link": link,
                                        "price": price.strip()
                                    })

                        print(f"ğŸ” ìˆ˜ì§‘ëœ ìƒí’ˆ ìˆ˜: {len(product_list)}ê°œ")

                        for product in product_list:
                            try:
                                if len(seen_links) >= MAX_COUNT:
                                    print(f"ğŸ›‘ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜({MAX_COUNT}) ë„ë‹¬ â†’ {category_name} ì¢…ë£Œ")
                                    await browser.close()
                                    return

                                print(f"ğŸ‘‰ ìƒì„¸ í˜ì´ì§€ ì´ë™ ì‹œë„: {product['link']}")
                                response = await detail_page.goto(product["link"])
                                if not response or response.status != 200:
                                    print(f"â— ìƒì„¸í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨ (ì‘ë‹µ ì—†ìŒ ë˜ëŠ” ë¹„ì •ìƒ): {product['link']}")
                                    continue

                                await detail_page.wait_for_timeout(2000)

                                description = "ìƒì„¸ì„¤ëª… ì—†ìŒ"
                                product_category = "ì•Œ ìˆ˜ ì—†ìŒ"
                                try:
                                    await detail_page.wait_for_selector("div.spec_list > div.items", timeout=3000)
                                    desc_el = await detail_page.query_selector("div.spec_list > div.items")
                                    if desc_el:
                                        description = await desc_el.inner_text()
                                        print(f"ğŸ“ ìƒì„¸ ì„¤ëª… ìˆ˜ì§‘ ì„±ê³µ: {description[:60]}...")
                                        if "/" in description:
                                            product_category = description.split("/")[0].strip()
                                    else:
                                        print("â— desc_elì´ Noneì„")
                                except Exception as e:
                                    print(f"â— ìƒì„¸ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")

                                try:
                                    await detail_page.click("a#danawa-prodBlog-companyReview-button-tab-companyReview")
                                    await detail_page.wait_for_timeout(1500)
                                except:
                                    pass

                                review_list = []
                                try:
                                    current_page = 1
                                    while current_page <= 3:
                                        await detail_page.wait_for_timeout(1000)
                                        review_elements = await detail_page.query_selector_all("div.atc")
                                        for el in review_elements:
                                            text = await el.inner_text()
                                            if text.strip():
                                                review_list.append(text.strip())
                                                if len(review_list) >= 20:
                                                    break
                                        if len(review_list) >= 20:
                                            break
                                        next_selector = f'a.page_num[data-pagenumber="{current_page + 1}"]'
                                        next_btn = await detail_page.query_selector(next_selector)
                                        if next_btn:
                                            await next_btn.click()
                                            current_page += 1
                                            await detail_page.wait_for_selector("div.atc")
                                        else:
                                            break
                                except Exception as e:
                                    print(f"â— ë¦¬ë·° í˜ì´ì§• ì˜¤ë¥˜: {e}")

                                result = {
                                    "ì œí’ˆì¹´í…Œê³ ë¦¬": product_category,
                                    "ì œí’ˆëª…": product["title"],
                                    "ë§í¬": product["link"],
                                    "ê°€ê²©": product["price"],
                                    "ìƒì„¸ì„¤ëª…": description[:1000],
                                    "ë¦¬ë·°": review_list if review_list else ["ë¦¬ë·° ì—†ìŒ"]
                                }

                                results.append(result)
                                print(f"âœ… ì €ì¥ ì™„ë£Œ ({len(seen_links)}ê°œ ëˆ„ì ): {product['title']}")

                                if len(results) >= 100:
                                    file_index += 1
                                    filename = f"{base_filename}_{file_index}.json"
                                    with open(filename, "w", encoding="utf-8") as f:
                                        json.dump(results, f, ensure_ascii=False, indent=4)
                                    print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ â†’ {filename}")
                                    results = []

                                time.sleep(1)

                            except Exception as e:
                                print(f"â— ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                                continue
                except Exception as e:
                    print(f"â— {i}í˜ì´ì§€ ì˜¤ë¥˜: {e}")

            try:
                next_btn = await page.query_selector('a.edge_nav.nav_next')
                if next_btn and await next_btn.is_visible():
                    await next_btn.click()
                    await page.wait_for_timeout(2000)
                else:
                    print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ì„¸íŠ¸ ë„ë‹¬")
                    break
            except Exception as e:
                print(f"â— ë‹¤ìŒ ì„¸íŠ¸ ì´ë™ ì‹¤íŒ¨: {e}")
                break

        await browser.close()

        if results:
            file_index += 1
            filename = f"{base_filename}_{file_index}_last.json"
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            print(f"âœ… ë§ˆì§€ë§‰ ì €ì¥ ì™„ë£Œ â†’ {filename}")

async def main():
    for category in CATEGORY_LIST:
        await scrape_category(category["id"], category["name"])

# â–¶ ì‹¤í–‰
asyncio.run(main())